{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Final.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TMIVJX8adDK2"
      },
      "source": [
        "##Final Submission With Two Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E8Lr-KmW7O88"
      },
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import pylab \n",
        "import scipy.stats as stats\n",
        "import datetime\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "from tqdm import tqdm \n",
        "import gc\n",
        "from sklearn.model_selection import StratifiedKFold, KFold\n",
        "from sklearn.metrics import log_loss\n",
        "import lightgbm\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.linear_model import LogisticRegression, Ridge\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from prettytable import PrettyTable\n",
        "import prettytable\n",
        "import pickle\n",
        "from sklearn.model_selection import GridSearchCV, cross_val_score, cross_val_predict, train_test_split\n",
        "from tqdm import tqdm"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XxVdt0kU7gAr"
      },
      "source": [
        "def reduce_mem_usage(df, verbose=True):\n",
        "  '''This function helps reduce memmory occuipied by \n",
        "  convertig the value to its rqquired data type'''\n",
        "\n",
        "  numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
        "  start_mem = df.memory_usage().sum() / 1024**2    \n",
        "  for col in df.columns:\n",
        "      col_type = df[col].dtypes\n",
        "      if col_type in numerics:\n",
        "          c_min = df[col].min()\n",
        "          c_max = df[col].max()\n",
        "          if str(col_type)[:3] == 'int':\n",
        "              if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
        "                  df[col] = df[col].astype(np.int8)\n",
        "              elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
        "                  df[col] = df[col].astype(np.int16)\n",
        "              elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
        "                  df[col] = df[col].astype(np.int32)\n",
        "              elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
        "                  df[col] = df[col].astype(np.int64)  \n",
        "          else:\n",
        "              if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
        "                  df[col] = df[col].astype(np.float16)\n",
        "              elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
        "                  df[col] = df[col].astype(np.float32)\n",
        "              else:\n",
        "                  df[col] = df[col].astype(np.float64)    \n",
        "  end_mem = df.memory_usage().sum() / 1024**2\n",
        "  if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
        "  return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VgrryNJ47huN"
      },
      "source": [
        "from tqdm import tqdm\n",
        "def oneHotEncoding_mod(df, features,transaction):\n",
        "  hist_transac_dict = {'category_2':[1.0,2.0,3.0,4.0,5.0], 'category_3':[1.0,2.0,3.0], 'month_lag':[0,-1,-2,-3,-4,-5,-6,-7,-8,-9,-10,-11,-12,-13]} \n",
        "  new_transac_dict = {'category_2':[1.0,2.0,3.0,4.0,5.0], 'category_3':[1.0,2.0,3.0], 'month_lag':[1,2]} \n",
        "\n",
        "  for feat in tqdm(features):\n",
        "    if (transaction=='hist'):\n",
        "      dict_=hist_transac_dict\n",
        "    else:\n",
        "      dict_=new_transac_dict\n",
        "      \n",
        "    unique_values =dict_[feat]\n",
        "\n",
        "    for cat in unique_values:\n",
        "      df[feat+'={}'.format(cat)] = (df[feat] == cat).astype(int)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YuN7U97nT5Me"
      },
      "source": [
        "def fillup_null_inf_values(train,df_train_columns):\n",
        "  for col in df_train_columns:\n",
        "    train[col].fillna(train[col].median(),inplace=True)\n",
        "\n",
        "  train = train.replace(np.inf, np.nan)\n",
        "  col=['new_hist_purchase_amount_sum','new_hist_purchase_amount_min','new_hist_purchase_amount_mean',\n",
        "     'new_hist_purchase_amount_max','new_hist_purchase_amount_var','new_hist_purchase_amount_diff','purchase_amount_total']\n",
        "\n",
        "  for c in col:\n",
        "    train[c].fillna(train[c].median(),inplace=True)\n",
        "  return train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NfnOAcBR9cP3"
      },
      "source": [
        "def get_new_columns(name,aggs):\n",
        "    return [name + '_' + k + '_' + agg for k in aggs.keys() for agg in aggs[k]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J0mfRLkW-nkY"
      },
      "source": [
        "## reference: https://towardsdatascience.com/find-your-best-customers-with-customer-segmentation-in-python-61d602f9eee6\n",
        "\n",
        "def RScore(x,p,d):\n",
        "    if x <= d[p][0.011]:\n",
        "        return 1\n",
        "    elif x <= d[p][0.050]:\n",
        "        return 2\n",
        "    elif x <= d[p][0.25]: \n",
        "        return 3\n",
        "    elif x <= d[p][0.5]:\n",
        "        return 4\n",
        "    elif x <= d[p][0.75]:\n",
        "        return 5\n",
        "    elif x <= d[p][0.95]:\n",
        "        return 6\n",
        "    elif x <= d[p][0.989]:\n",
        "        return 7\n",
        "    else:\n",
        "        return 8\n",
        "    \n",
        "def FMScore(x,p,d):\n",
        "    if x <= d[p][0.011]:\n",
        "        return 8\n",
        "    elif x <= d[p][0.050]:\n",
        "        return 7\n",
        "    elif x <= d[p][0.25]: \n",
        "        return 6\n",
        "    elif x <= d[p][0.5]:\n",
        "        return 5\n",
        "    elif x <= d[p][0.75]:\n",
        "        return 4\n",
        "    elif x <= d[p][0.95]:\n",
        "        return 3\n",
        "    elif x <= d[p][0.989]:\n",
        "        return 2\n",
        "    else:\n",
        "        return 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GjtCa1GN-zRH"
      },
      "source": [
        "def rfm(df,quantiles, transc):\n",
        "  ## grouping quantiles\n",
        "  df[transc+'r_quantile'] = df[transc+'purchase_recency'].apply(RScore, args=(transc+'purchase_recency',quantiles))\n",
        "  df[transc+'f_quantile'] = df[transc+'card_id_size'].apply(FMScore, args=(transc+'card_id_size',quantiles))\n",
        "  df[transc+'m_quantile'] = df[transc+'purchase_amount_sum'].apply(FMScore, args=(transc+'purchase_amount_sum',quantiles))\n",
        "  ## calaculating RFM index and RFMScore\n",
        "  df[transc+'RFMindex'] = df[transc+'r_quantile'].map(str)+df[transc+'f_quantile'].map(str)+df[transc+'m_quantile'].map(str)                       \n",
        "  df[transc+'RFMScore'] = df[transc+'r_quantile']+df[transc+'f_quantile']+df[transc+'m_quantile']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X3xAHvyDJxtn"
      },
      "source": [
        "#definind the rmse metric\n",
        "import keras.backend as K\n",
        "def rmse(y_true, y_pred):\n",
        "    return K.sqrt(K.mean(K.square(y_pred - y_true), axis=-1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Ce4r5DNIdxe"
      },
      "source": [
        "test=pd.read_csv(\"/content/drive/MyDrive/CASE_STUDIES/CASE_STUDY_1/DATA/test_engineered_4_Jul_2021.csv\",nrows=10000)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BTs_c6cE8PX8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "514a4ced-eccf-4d3f-b322-682f74cb957b"
      },
      "source": [
        "df_test_columns = [c for c in test.columns if c not in ['card_id', 'first_active_month', 'target', 'outliers']]  \n",
        "card_dict=dict()\n",
        "for c_id in tqdm(test['card_id']):\n",
        "  tmp_df=test[test['card_id']==c_id][df_test_columns]\n",
        "  tmp_df=tmp_df.values.tolist()[0]\n",
        "  card_dict[c_id]=tmp_df\n",
        "  #card_dict\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 10000/10000 [00:26<00:00, 383.44it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qL6FObogHUI1"
      },
      "source": [
        "a_file = open(\"/content/drive/MyDrive/CASE_STUDIES/CASE_STUDY_1/DATA/card_dict_top_10K.pkl\", \"wb\")\n",
        "pickle.dump(card_dict, a_file)\n",
        "a_file.close()"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "odS_VvZbZ3yE"
      },
      "source": [
        "card_dict_file = open(\"/content/drive/MyDrive/CASE_STUDIES/CASE_STUDY_1/DATA/card_dict_top_10K.pkl\", \"rb\")\n",
        "card_dict = pickle.load(card_dict_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h2qeYs-cILiB"
      },
      "source": [
        "##final_fun_1():\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MfI9m0cC8O_c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb01268d-7aff-4eca-fd65-8cb15d19a943"
      },
      "source": [
        "%%writefile EMCR.py\n",
        "import pickle\n",
        "import numpy as np\n",
        "import streamlit as st\n",
        "import lightgbm\n",
        "\n",
        "st.title(\"Elo Merchant Category Recommendation - Kaggle Competition with Loyalty Score Predictions\")\n",
        "\n",
        "card_dict_file = open(\"/content/drive/MyDrive/CASE_STUDIES/CASE_STUDY_1/DATA/card_dict.pkl\", \"rb\")\n",
        "card_dict = pickle.load(card_dict_file)\n",
        "Pkl_Filename = \"/content/drive/MyDrive/CASE_STUDIES/CASE_STUDY_1/DATA/LGB_Model_61579.pkl\"  \n",
        "\n",
        "def final_fun_1(card_id='C_ID_0ab67a22ab'):\n",
        "  card_data=card_dict.get(card_id)\n",
        "  card_data=np.array(card_data).reshape((1,-1))\n",
        "  pred_y_test_pickle=lightbgm_reg_pickle.predict(card_data)\n",
        "  return pred_y_test_pickle[0]\n",
        "\n",
        "# Load the Model back from file \n",
        "with open(Pkl_Filename, 'rb') as file:\n",
        "  lightbgm_reg_pickle = pickle.load(file)\n",
        "  card_select = st.selectbox('Select Card ID: ',card_dict.keys())\n",
        "  card_selection_msg=st.text('You have selected card: '+card_select)\n",
        "  if st.button('Predict Loyalty Score'):\n",
        "    loyalty_score=final_fun_1(card_select)\n",
        "    st.write('Card Id: '+card_select)\n",
        "    st.write(\"Loylty Score: \"+str(loyalty_score))\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing EMCR.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W1AtDXTJIj7c"
      },
      "source": [
        "##final_fun_2():"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TjjMC_mrEIih"
      },
      "source": [
        "def final_fun_2(train,historical_transactions,new_merchant_transctions):\n",
        "  #one hot encoding\n",
        "  categorical_features = ['category_2','category_3','month_lag']\n",
        "  oneHotEncoding_mod(historical_transactions, features=categorical_features,transaction='hist')\n",
        "  oneHotEncoding_mod(new_merchant_transctions, features=categorical_features,transaction='new')\n",
        "  \n",
        "  new_merchant_transctions['purchase_amount'] = np.round(new_merchant_transctions['purchase_amount'] / 0.00150265118 + 497.06, 2)\n",
        "  historical_transactions['purchase_amount'] = np.round(historical_transactions['purchase_amount'] / 0.00150265118 + 497.06, 2)\n",
        "  historical_transactions=reduce_mem_usage(historical_transactions)\n",
        "  new_merchant_transctions=reduce_mem_usage(new_merchant_transctions)\n",
        "  gc.collect()\n",
        "  for df in [historical_transactions,new_merchant_transctions]:\n",
        "    df['purchase_date'] = pd.to_datetime(df['purchase_date'])\n",
        "    df['year'] = df['purchase_date'].dt.year\n",
        "    df['day']=df['purchase_date'].dt.day\n",
        "    df['weekofyear'] = df['purchase_date'].dt.weekofyear\n",
        "    df['month'] = df['purchase_date'].dt.month\n",
        "    df['dayofweek'] = df['purchase_date'].dt.dayofweek\n",
        "    df['weekend'] = (df.purchase_date.dt.weekday >=5).astype(int)\n",
        "    df['hour'] = df['purchase_date'].dt.hour\n",
        "    df['month_diff'] = ((datetime.datetime.today() - df['purchase_date']).dt.days)//30\n",
        "    df['month_diff'] += df['month_lag']\n",
        "    df['installments']=df['installments'].map({-1:14, 0:0,1:1,2:2,3:3,4:4,5:5,6:6,7:7,8:8,9:9,10:10,11:11,12:12,999:13})\n",
        "    df['price']=df['purchase_amount']/df['installments']\n",
        "    df['purchase_amount_quantiles']= pd.qcut(df['purchase_amount'], 5, labels=False)\n",
        "    df['installments_quantiles']= pd.qcut(df['installments'], 5, labels=False,duplicates='drop')\n",
        "    df['amount_month_ratio'] = df['purchase_amount'].values / (1.0 + df['month_diff'].values)\n",
        "    df=reduce_mem_usage(df)\n",
        "    gc.collect()\n",
        "  #Holidays features\n",
        "  holidays = {'EasterDay_2017' : '2017-04-16','AllSoulsDay_2017': '2017-11-2','ChristmasDay_2017': '2017-12-25','FathersDay_2017': '2017-08-13',\n",
        "            'ChildrenDay_2017':'2017-10-12','BlackFriday_2017':'2017-11-24','ValentineDay_2017':'2017-06-12','MothersDay_2018':'2018-05-13'}\n",
        "  for day, date in holidays.items():\n",
        "    new_merchant_transctions[day] = (pd.to_datetime(date) - new_merchant_transctions['purchase_date']).dt.days\n",
        "    new_merchant_transctions[day] = new_merchant_transctions[day].apply(lambda x: x if x > 0 and x < 30 else 0)\n",
        "    historical_transactions[day] = (pd.to_datetime(date) - historical_transactions['purchase_date']).dt.days\n",
        "    historical_transactions[day] = historical_transactions[day].apply(lambda x: x if x > 0 and x < 30 else 0)\n",
        "  \n",
        "  \n",
        "  aggs = {}\n",
        "  for col in ['month','hour','day','weekend','weekofyear','dayofweek','year','subsector_id','merchant_id','merchant_category_id','price','city_id','state_id',\n",
        "            'category_2=1.0', 'category_2=3.0', 'category_2=5.0',\n",
        "            'category_2=2.0', 'category_2=4.0', 'category_3=1.0', 'category_3=2.0',\n",
        "            'category_3=3.0', 'month_lag=-8', 'month_lag=-7', 'month_lag=-6',\n",
        "            'month_lag=-5', 'month_lag=-11', 'month_lag=0', 'month_lag=-3',\n",
        "            'month_lag=-9', 'month_lag=-4', 'month_lag=-1', 'month_lag=-13',\n",
        "            'month_lag=-10', 'month_lag=-12', 'month_lag=-2']:\n",
        "      aggs[col] = ['nunique']\n",
        "\n",
        "  aggs['EasterDay_2017']=['sum', 'mean']\n",
        "  aggs['AllSoulsDay_2017']=['sum', 'mean']\n",
        "  aggs['ChristmasDay_2017']=['sum', 'mean']\n",
        "  aggs['FathersDay_2017']=['sum', 'mean']\n",
        "  aggs['ChildrenDay_2017']=['sum', 'mean']\n",
        "  aggs['BlackFriday_2017']=['sum', 'mean']\n",
        "  aggs['ValentineDay_2017']=['sum', 'mean']\n",
        "  aggs['MothersDay_2018']=['sum', 'mean']\n",
        "\n",
        "  aggs['category_2=1.0']=['sum', 'mean']\n",
        "  aggs['category_2=3.0']=['sum', 'mean']\n",
        "  aggs['category_2=2.0']=['sum', 'mean']\n",
        "  aggs['category_2=4.0']=['sum', 'mean']\n",
        "  aggs['category_2=5.0']=['sum', 'mean']\n",
        "  aggs['category_3=1.0']=['sum', 'mean']\n",
        "  aggs['category_3=2.0']=['sum', 'mean']\n",
        "  aggs['category_3=3.0']=['sum', 'mean']\n",
        "\n",
        "  aggs['month_lag=0']= ['sum','mean']\n",
        "  aggs['month_lag=-1']=['sum','mean']\n",
        "  aggs['month_lag=-2']=['sum','mean']\n",
        "\n",
        "  aggs['purchase_amount'] = ['sum','max','min','mean','var']\n",
        "  aggs['installments'] = ['sum','max','min','mean','var','std','skew']\n",
        "  aggs['purchase_date'] = ['max','min']\n",
        "  aggs['month_lag'] = ['mean', 'std', 'min', 'max', 'skew']\n",
        "  aggs['month_diff'] = ['mean']\n",
        "  aggs['authorized_flag'] = ['sum', 'mean']\n",
        "  aggs['weekend'] = ['sum', 'mean']\n",
        "  aggs['category_1'] = ['sum', 'mean']\n",
        "  aggs['card_id'] = ['size']\n",
        "  aggs['installments_quantiles']=['var', 'mean', 'skew']\n",
        "  aggs['purchase_amount_quantiles']=['var', 'mean', 'skew']\n",
        "  aggs['amount_month_ratio']=['mean', 'std', 'min', 'max', 'skew']\n",
        "  #aggs['price'] = ['sum', 'mean']  \n",
        "  aggs['weekofyear']=['mean', 'min', 'max']\n",
        "  aggs['dayofweek']=['mean', 'min', 'max']\n",
        "  aggs['hour']=['mean', 'min', 'max']  \n",
        "    \n",
        "  new_columns = get_new_columns('hist',aggs)\n",
        "  df_hist_trans_group = historical_transactions.groupby('card_id').agg(aggs)\n",
        "  df_hist_trans_group.columns = new_columns\n",
        "  df_hist_trans_group.reset_index(drop=False,inplace=True)\n",
        "\n",
        "  df_hist_trans_group['hist_purchase_date_diff'] = (df_hist_trans_group['hist_purchase_date_max'] - df_hist_trans_group['hist_purchase_date_min']).dt.days\n",
        "  df_hist_trans_group['hist_purchase_date_average'] = df_hist_trans_group['hist_purchase_date_diff']/df_hist_trans_group['hist_card_id_size']\n",
        "  df_hist_trans_group['hist_purchase_date_uptonow'] = (datetime.datetime.today() - df_hist_trans_group['hist_purchase_date_max']).dt.days\n",
        "  df_hist_trans_group['hist_of_hist_purchase_date_average']=df_hist_trans_group['hist_purchase_date_diff']*df_hist_trans_group['hist_purchase_date_average']\n",
        "\n",
        "  #Newly added features\n",
        "  df_hist_trans_group['hist_purchase_amount_diff'] = df_hist_trans_group['hist_purchase_amount_max'].values - df_hist_trans_group['hist_purchase_amount_min'].values\n",
        "  df_hist_trans_group['hist_purchase_count_ratio'] = df_hist_trans_group['hist_card_id_size'].values / (1.0 + df_hist_trans_group['hist_purchase_date_diff'].values)\n",
        "  df_hist_trans_group['hist_purchase_recency'] = (datetime.datetime.today() - pd.to_datetime(df_hist_trans_group['hist_purchase_date_max']))/(24*np.timedelta64(1, 'h'))\n",
        "\n",
        "  df_hist_trans_group['hist_category_2_pa_mean'] = historical_transactions.groupby('category_2')['purchase_amount'].transform('mean')\n",
        "  df_hist_trans_group['hist_category_3_pa_mean'] = historical_transactions.groupby('category_3')['purchase_amount'].transform('mean')\n",
        "\n",
        "  df_hist_trans_group['hist_merchant_id_count_mean'] = df_hist_trans_group['hist_card_id_size'].values / (1.0+df_hist_trans_group['hist_merchant_id_nunique'].values)\n",
        "  df_hist_trans_group['hist_month_lag_0_-1_ratio'] = df_hist_trans_group['hist_month_lag=0_sum']/ (1.0+ df_hist_trans_group['hist_month_lag=-1_sum'])\n",
        "  df_hist_trans_group['hist_month_lag_0_-2_ratio'] = df_hist_trans_group['hist_month_lag=0_sum'] / (1.0+ df_hist_trans_group['hist_month_lag=-2_sum'])\n",
        "\n",
        "  train = train.merge(df_hist_trans_group,on='card_id',how='left')\n",
        "  del df_hist_trans_group\n",
        "  gc.collect()\n",
        "\n",
        "\n",
        "  aggs = {}\n",
        "  for col in ['month','hour','day','weekend','weekofyear','dayofweek','year','subsector_id','merchant_id','merchant_category_id','price','city_id','state_id',\n",
        "            'category_2=1.0', 'category_2=3.0', 'category_2=2.0',\n",
        "            'category_2=4.0', 'category_2=5.0', 'category_3=2.0', 'category_3=1.0',\n",
        "            'category_3=3.0', 'month_lag=1', 'month_lag=2']:\n",
        "      aggs[col] = ['nunique']\n",
        "\n",
        "  aggs['EasterDay_2017']=['sum', 'mean']\n",
        "  aggs['AllSoulsDay_2017']=['sum', 'mean']\n",
        "  aggs['ChristmasDay_2017']=['sum', 'mean']\n",
        "  aggs['FathersDay_2017']=['sum', 'mean']\n",
        "  aggs['ChildrenDay_2017']=['sum', 'mean']\n",
        "  aggs['BlackFriday_2017']=['sum', 'mean']\n",
        "  aggs['ValentineDay_2017']=['sum', 'mean']\n",
        "  aggs['MothersDay_2018']=['sum', 'mean']\n",
        "\n",
        "  aggs['category_2=1.0']=['sum', 'mean']\n",
        "  aggs['category_2=3.0']=['sum', 'mean']\n",
        "  aggs['category_2=2.0']=['sum', 'mean']\n",
        "  aggs['category_2=4.0']=['sum', 'mean']\n",
        "  aggs['category_2=5.0']=['sum', 'mean']\n",
        "  aggs['category_3=1.0']=['sum', 'mean']\n",
        "  aggs['category_3=2.0']=['sum', 'mean']\n",
        "  aggs['category_3=3.0']=['sum', 'mean']\n",
        "\n",
        "  aggs['month_lag=1']= ['sum','mean']\n",
        "  aggs['month_lag=2']=['sum','mean']\n",
        "\n",
        "  aggs['purchase_amount'] = ['sum','max','min','mean','var']\n",
        "  aggs['installments'] = ['sum','max','min','mean','var','std','skew']\n",
        "  aggs['purchase_date'] = ['max','min']\n",
        "  aggs['month_lag'] = ['mean', 'std', 'min', 'max', 'skew']\n",
        "  aggs['month_diff'] = ['mean']\n",
        "  aggs['authorized_flag'] = ['sum', 'mean']\n",
        "  aggs['weekend'] = ['sum', 'mean']\n",
        "  aggs['category_1'] = ['sum', 'mean']\n",
        "  aggs['card_id'] = ['size']\n",
        "  aggs['installments_quantiles']=['var', 'mean', 'skew']\n",
        "  aggs['purchase_amount_quantiles']=['var', 'mean', 'skew']\n",
        "  #aggs['price'] = ['sum', 'mean']\n",
        "  aggs['weekofyear']=['mean', 'min', 'max']\n",
        "  aggs['dayofweek']=['mean', 'min', 'max']\n",
        "  aggs['hour']=['mean', 'min', 'max']\n",
        "\n",
        "  new_columns = get_new_columns('new_hist',aggs)\n",
        "  df_hist_trans_group = new_merchant_transctions.groupby('card_id').agg(aggs)\n",
        "  df_hist_trans_group.columns = new_columns\n",
        "  df_hist_trans_group.reset_index(drop=False,inplace=True)\n",
        "  df_hist_trans_group['new_hist_purchase_date_diff'] = (df_hist_trans_group['new_hist_purchase_date_max'] - df_hist_trans_group['new_hist_purchase_date_min']).dt.days\n",
        "  df_hist_trans_group['new_hist_purchase_date_average'] = df_hist_trans_group['new_hist_purchase_date_diff']/df_hist_trans_group['new_hist_card_id_size']\n",
        "  df_hist_trans_group['new_hist_purchase_date_uptonow'] = (datetime.datetime.today() - df_hist_trans_group['new_hist_purchase_date_max']).dt.days\n",
        "  df_hist_trans_group['new_hist_of_new_hist_purchase_date_average']=df_hist_trans_group['new_hist_purchase_date_diff']*df_hist_trans_group['new_hist_purchase_date_average']\n",
        "\n",
        "  #newly added fature\n",
        "  df_hist_trans_group['new_hist_purchase_amount_diff'] = df_hist_trans_group['new_hist_purchase_amount_max'].values - df_hist_trans_group['new_hist_purchase_amount_min'].values\n",
        "  df_hist_trans_group['new_hist_purchase_count_ratio'] = df_hist_trans_group['new_hist_card_id_size'].values / (1.0 + df_hist_trans_group['new_hist_purchase_date_diff'].values)\n",
        "  df_hist_trans_group['new_hist_purchase_recency'] = (datetime.datetime.today() - pd.to_datetime(df_hist_trans_group['new_hist_purchase_date_max']))/(24*np.timedelta64(1, 'h'))\n",
        "\n",
        "  df_hist_trans_group['new_hist_category_2_pa_mean'] = historical_transactions.groupby('category_2')['purchase_amount'].transform('mean')\n",
        "  df_hist_trans_group['new_hist_category_3_pa_mean'] = historical_transactions.groupby('category_3')['purchase_amount'].transform('mean')\n",
        "\n",
        "  df_hist_trans_group['new_hist_merchant_id_count_mean'] = df_hist_trans_group['new_hist_card_id_size'].values / (1.0+df_hist_trans_group['new_hist_merchant_id_nunique'].values)\n",
        "  df_hist_trans_group['new_hist_month_lag_1_2_ratio'] = df_hist_trans_group['new_hist_month_lag=1_sum'] / (1.0+ df_hist_trans_group['new_hist_month_lag=2_sum'])\n",
        "\n",
        "  train = train.merge(df_hist_trans_group,on='card_id',how='left')\n",
        "  del df_hist_trans_group;\n",
        "  gc.collect()\n",
        "\n",
        "  \n",
        "  ## qunatiles of RFM\n",
        "  quantiles_new = train[['new_hist_purchase_recency','new_hist_card_id_size','new_hist_purchase_amount_sum']].quantile(q=[0.011,0.05,0.25,0.5,0.75,0.95,0.989]).to_dict()\n",
        "  quantiles_hist = train[['hist_purchase_recency','hist_card_id_size','hist_purchase_amount_sum']].quantile(q=[0.011,0.05,0.25,0.5,0.75,0.95,0.989]).to_dict()\n",
        "  \n",
        "  train['outliers'] = 0\n",
        "  train.loc[train['target'] < -30, 'outliers'] = 1\n",
        "\n",
        "  for df in [train]:\n",
        "    rfm(df,quantiles_new,transc = 'new_hist_')\n",
        "    rfm(df,quantiles_hist,transc = 'hist_')\n",
        "    df['new_hist_RFMindex']=df[\"new_hist_RFMindex\"].map(int)\n",
        "    df[\"hist_RFMindex\"]=df[\"hist_RFMindex\"].map(int)\n",
        "    df['first_active_month'] = pd.to_datetime(df['first_active_month'])\n",
        "    df['dayofweek'] = df['first_active_month'].dt.dayofweek\n",
        "    df['weekofyear'] = df['first_active_month'].dt.weekofyear\n",
        "    df['month'] = df['first_active_month'].dt.month\n",
        "    df['elapsed_time'] = (datetime.datetime.today() - df['first_active_month']).dt.days\n",
        "    df['hist_first_buy'] = (df['hist_purchase_date_min'] - df['first_active_month']).dt.days\n",
        "    df['new_hist_first_buy'] = (df['new_hist_purchase_date_min'] - df['first_active_month']).dt.days\n",
        "    df['card_id_total'] = df['new_hist_card_id_size']+df['hist_card_id_size']\n",
        "    df['purchase_amount_total'] = df['new_hist_purchase_amount_sum']+df['hist_purchase_amount_sum']\n",
        "    df['days_feature1'] = df['elapsed_time'] * df['feature_1']\n",
        "\n",
        "    for f in ['hist_purchase_date_max','hist_purchase_date_min','new_hist_purchase_date_max','new_hist_purchase_date_min']:\n",
        "      df[f] = df[f].fillna(df[f].mean())\n",
        "      df[f] = df[f].astype(np.int64) * 1e-9\n",
        "    \n",
        "    for f in ['feature_1','feature_2','feature_3']:\n",
        "      order_label = train.groupby([f])['outliers'].mean()\n",
        "      df[f] = df[f].map(order_label)\n",
        "\n",
        "\n",
        "  df_train_columns = [c for c in train.columns if c not in ['card_id', 'first_active_month', 'target', 'outliers']]\n",
        "  train=fillup_null_inf_values(train,df_train_columns)\n",
        "  \n",
        "  y = train['target']\n",
        "  X = train[df_train_columns]\n",
        "  X_train_all, X_test_com, y_train_all, y_test_com = train_test_split(X, y, test_size=0.20, random_state=42)\n",
        " \n",
        "  Pkl_Filename = \"/content/drive/MyDrive/CASE_STUDIES/CASE_STUDY_1/DATA/LGB_Model_61579.pkl\"  \n",
        "  # Load the Model back from file\n",
        "  with open(Pkl_Filename, 'rb') as file:\n",
        "    lightbgm_reg_pickle = pickle.load(file)\n",
        "\n",
        "  pred_y_test_pickle=lightbgm_reg_pickle.predict(X_test_com)\n",
        "  print(\"RMSE Score::{:.3f}\".format(rmse(y_test_com, pred_y_test_pickle)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dUwYJSI6L9hP",
        "outputId": "efd0878c-3360-4fb7-f4ff-b8f0b8e01d88"
      },
      "source": [
        "#Loading all data files from goole drive\n",
        "train = reduce_mem_usage(pd.read_csv('/content/drive/MyDrive/CASE_STUDIES/CASE_STUDY_1/DATA/train.csv', parse_dates=[\"first_active_month\"]))\n",
        "historical_transactions=reduce_mem_usage(pd.read_csv(\"/content/drive/MyDrive/CASE_STUDIES/CASE_STUDY_1/DATA/historical_transactions_model_imputed.csv\",parse_dates=['purchase_date']))\n",
        "new_merchant_transctions=reduce_mem_usage(pd.read_csv(\"/content/drive/MyDrive/CASE_STUDIES/CASE_STUDY_1/DATA/new_merchant_transctions_model_imputed.csv\",parse_dates=['purchase_date']))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mem. usage decreased to  4.04 Mb (56.2% reduction)\n",
            "Mem. usage decreased to 1193.84 Mb (61.6% reduction)\n",
            "Mem. usage decreased to 76.76 Mb (63.4% reduction)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Py9OLcH5MMN5",
        "outputId": "ffa2cfb4-cc90-497e-f8f6-1b50d3efda24"
      },
      "source": [
        "final_fun_2(train,historical_transactions,new_merchant_transctions)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 3/3 [00:04<00:00,  1.66s/it]\n",
            "100%|██████████| 3/3 [00:00<00:00, 11.16it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Mem. usage decreased to 1804.64 Mb (70.9% reduction)\n",
            "Mem. usage decreased to 106.71 Mb (52.9% reduction)\n",
            "Mem. usage decreased to 2415.44 Mb (46.6% reduction)\n",
            "Mem. usage decreased to 155.38 Mb (47.8% reduction)\n",
            "RMSE Score::3.590\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}